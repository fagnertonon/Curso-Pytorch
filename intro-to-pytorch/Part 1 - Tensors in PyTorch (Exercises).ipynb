{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning with PyTorch\n",
    "\n",
    "Neste caderno, você será apresentado ao [PyTorch] (http://pytorch.org/), uma estrutura para construir e treinar redes neurais. O PyTorch se comporta de várias maneiras como as matrizes que você ama do Numpy. Afinal, essas matrizes Numpy são apenas tensores. O PyTorch pega esses tensores e facilita a sua transferência para GPUs para o processamento mais rápido necessário ao treinar redes neurais. Ele também fornece um módulo que calcula automaticamente gradientes (para retropropagação!) E outro módulo especificamente para a construção de redes neurais. No conjunto, o PyTorch acaba sendo mais coerente com o Python e a pilha Numpy / Scipy em comparação com o TensorFlow e outras estruturas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "O Deep Learning é baseado em redes neurais artificiais que existem de alguma forma desde o final da década de 1950. As redes são construídas a partir de partes individuais que se aproximam dos neurônios, normalmente chamadas de unidades ou simplesmente \"neurônios\". Cada unidade possui algum número de entradas ponderadas. Essas entradas ponderadas são somadas (uma combinação linear) e depois passadas por uma função de ativação para obter a saída da unidade.\n",
    "\n",
    "<img src=\"assets/simple_neuron.png\" width=400px>\n",
    "\n",
    "Matematicamente, isso se parece com:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f(w_1 x_1 + w_2 x_2 + b) \\\\\n",
    "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Com vetores, este é o produto interno / ponto de dois vetores:\n",
    "\n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots  x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_1 \\\\\n",
    "           w_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           w_n\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "Acontece que os cálculos de redes neurais são apenas um monte de operações de álgebra linear em * tensores *, uma generalização de matrizes. Um vetor é um tensor unidimensional, uma matriz é um tensor bidimensional, uma matriz com três índices é um tensor tridimensional (imagens em cores RGB, por exemplo). A estrutura de dados fundamental para redes neurais são os tensores e o PyTorch (assim como praticamente qualquer outra estrutura de aprendizado profundo) é construído em torno dos tensores.\n",
    "\n",
    "<img src=\"assets/tensor_examples.svg\" width=600px>\n",
    "\n",
    "Com o básico abordado, é hora de explorar como podemos usar o PyTorch para construir uma rede neural simples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    \"\"\" Sigmoid activation function \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate some data --Gere alguns dados\n",
    "torch.manual_seed(7) # Set the random seed so things are predictable -- Defina a semente aleatória para que as coisas sejam previsíveis\n",
    "\n",
    "# Features are 5 random normal variables -- Os recursos são 5 variáveis normais aleatórias\n",
    "features = torch.randn((1, 5))\n",
    "# True weights for our data, random normal variables again -- Pesos reais para nossos dados, variáveis normais aleatórias novamente\n",
    "weights = torch.randn_like(features)\n",
    "# and a true bias term -- e um termo verdadeiro viés\n",
    "bias = torch.randn((1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acima, geramos dados que podemos usar para obter a saída de nossa rede simples. Por enquanto, tudo é aleatório. No futuro, começaremos a usar dados normais. Passando por cada linha relevante:\n",
    "\n",
    "`features = torch.randn ((1, 5))` cria um tensor com a forma `(1, 5)`, uma linha e cinco colunas, que contém valores distribuídos aleatoriamente de acordo com a distribuição normal, com média de zero e padrão desvio de um.\n",
    "\n",
    "`pesos = torch.randn_like (features)` cria outro tensor com a mesma forma que `features`, novamente contendo valores de uma distribuição normal.\n",
    "\n",
    "Finalmente, `bias = torch.randn ((1, 1))` cria um valor único a partir de uma distribuição normal.\n",
    "\n",
    "Os tensores PyTorch podem ser adicionados, multiplicados, subtraídos, etc., assim como os arrays Numpy. Em geral, você usará os tensores PyTorch da mesma maneira que usaria as matrizes Numpy. Eles trazem alguns benefícios interessantes, como a aceleração da GPU, que veremos mais adiante. Por enquanto, use os dados gerados para calcular a saída dessa rede simples de camada única.\n",
    "> ** Exercício **: Calcule a saída da rede com recursos de entrada `recursos`, pesos` pesos` e bias `bias`. Semelhante ao Numpy, o PyTorch possui a função [`torch.sum ()`] (https://pytorch.org/docs/stable/torch.html#torch.sum), além do método `.sum ()` em tensores, para somar somas. Use a função `ativação 'definida acima como a função de ativação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Calcule a saída desta rede usando os pesos e tensores de polarização\n",
    "y = activation(torch.sum(features *weights) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode fazer a multiplicação e somar na mesma operação usando uma multiplicação de matrizes. Em geral, convém usar multiplicações de matriz, pois elas são mais eficientes e aceleradas usando bibliotecas modernas e computação de alto desempenho em GPUs.\n",
    "\n",
    "Aqui, queremos fazer uma multiplicação matricial dos recursos e pesos. Para isso, podemos usar [`torch.mm ()`] (https://pytorch.org/docs/stable/torch.html#torch.mm) ou [`torch.matmul ()`] (https: // pytorch.org/docs/stable/torch.html#torch.matmul), que é um pouco mais complicado e suporta transmissão. Se tentarmos fazê-lo com os recursos e pesos como eles são, obteremos um erro\n",
    "\n",
    "```python\n",
    ">> torch.mm(features, weights)\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "RuntimeError                              Traceback (most recent call last)\n",
    "<ipython-input-13-15d592eb5279> in <module>()\n",
    "----> 1 torch.mm(features, weights)\n",
    "\n",
    "RuntimeError: size mismatch, m1: [1 x 5], m2: [1 x 5] at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524590658547/work/aten/src/TH/generic/THTensorMath.c:2033\n",
    "```\n",
    "\n",
    "Ao criar redes neurais em qualquer estrutura, você verá isso com frequência. Realmente frequentemente. O que está acontecendo aqui é que nossos tensores não têm as formas corretas para realizar uma multiplicação de matrizes. Lembre-se de que para multiplicações de matrizes, o número de colunas no primeiro tensor deve ser igual ao número de linhas na segunda coluna. Ambos os `recursos` e` pesos` têm a mesma forma, `(1, 5)`. Isso significa que precisamos alterar a forma dos pesos para que a multiplicação da matriz funcione.\n",
    "\n",
    "**Nota:** Para ver a forma de um tensor chamado `tensor`, use` tensor.shape`. Se você estiver construindo redes neurais, estará usando esse método frequentemente.\n",
    "\n",
    "Existem algumas opções aqui: [`weights.reshape ()`] (https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape), [`weights.resize _ ()`] ( https://pytorch.org/docs/stable/tensors.html#torch.Tensor.resize_) e [`weights.view ()`] (https://pytorch.org/docs/stable/tensors.html# tocha.Tensor.view).\n",
    "\n",
    "* `weights.reshape (a, b)` retornará um novo tensor com os mesmos dados que `pesos com tamanho` `(a, b)`as vezes e às vezes um clone, pois ele copia os dados para outra parte do memória.\n",
    "* `weights.resize_ (a, b)` retorna o mesmo tensor com uma forma diferente. No entanto, se a nova forma resultar em menos elementos que o tensor original, alguns elementos serão removidos do tensor (mas não da memória). Se a nova forma resultar em mais elementos que o tensor original, novos elementos serão não inicializados na memória. Aqui, devo observar que o sublinhado no final do método indica que esse método é realizado ** no local **. Aqui está um ótimo tópico do fórum para [leia mais sobre operações no local] (https://discuss.pytorch.org/t/what-is-in-place-operation/16244) no PyTorch.\n",
    "* `weights.view (a, b)` retornará um novo tensor com os mesmos dados que `weights` com tamanho` (a, b) `.\n",
    "\n",
    "Eu normalmente uso `.view ()`, mas qualquer um dos três métodos funcionará para isso. Portanto, agora podemos remodelar `pesos` 'para ter cinco linhas e uma coluna com algo como` pesos.view (5, 1) `.\n",
    "\n",
    "> **Exercício**: Calcule a saída de nossa pequena rede usando multiplicação de matrizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "torch.Size([1, 5])\n",
      "tensor([[0.1595]])\n"
     ]
    }
   ],
   "source": [
    "## Calculate the output of this network using matrix multiplication\n",
    "print(features.shape)\n",
    "print(weights.shape)\n",
    "\n",
    "y = activation(torch.mm(features, weights.view(5,1)) + bias)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack them up!\n",
    "\n",
    "É assim que você pode calcular a saída para um único neurônio. O poder real desse algoritmo acontece quando você começa a empilhar essas unidades individuais em camadas e pilhas de camadas, em uma rede de neurônios. A saída de uma camada de neurônios se torna a entrada para a próxima camada. Com várias unidades de entrada e unidades de saída, agora precisamos expressar os pesos como uma matriz.\n",
    "\n",
    "<img src='assets/multilayer_diagram_weights.png' width=450px>\n",
    "\n",
    "A primeira camada mostrada na parte inferior aqui são as entradas, compreensivelmente chamadas de **camada de entrada**. A camada do meio é chamada de **camada oculta** e a camada final (à direita) é a **camada de saída**. Podemos expressar essa rede matematicamente com matrizes novamente e usar a multiplicação de matrizes para obter combinações lineares para cada unidade em uma operação. Por exemplo, a camada oculta ($ h_1 $ e $ h_2 $ aqui) pode ser calculada \n",
    "\n",
    "$$\n",
    "\\vec{h} = [h_1 \\, h_2] = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots \\, x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_{11} & w_{12} \\\\\n",
    "           w_{21} &w_{22} \\\\\n",
    "           \\vdots &\\vdots \\\\\n",
    "           w_{n1} &w_{n2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "A saída para esta pequena rede é encontrada tratando a camada oculta como entradas para a unidade de saída. A saída da rede é expressa simplesmente\n",
    "\n",
    "$$\n",
    "y =  f_2 \\! \\left(\\, f_1 \\! \\left(\\vec{x} \\, \\mathbf{W_1}\\right) \\mathbf{W_2} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate some data\n",
    "torch.manual_seed(7) # Defina a semente aleatória para que as coisas sejam previsíveis\n",
    "\n",
    "# Os recursos são três variáveis normais aleatórias\n",
    "features = torch.randn((1, 3))\n",
    "\n",
    "# Defina o tamanho de cada camada em nossa rede\n",
    "n_input = features.shape[1]     # Número de unidades de entrada, deve corresponder ao número de recursos de entrada\n",
    "n_hidden = 2                    # Number of hidden units \n",
    "n_output = 1                    # Number of output units\n",
    "\n",
    "# Pesos para entradas na camada oculta\n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "# Pesos da camada oculta para a camada de saída\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "# e termos de polarização para camadas ocultas e de saída\n",
    "B1 = torch.randn((1, n_hidden))\n",
    "B2 = torch.randn((1, n_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Calculate the output for this multi-layer network using the weights `W1` & `W2`, and the biases, `B1` & `B2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did this correctly, you should see the output `tensor([[ 0.3171]])`.\n",
    "\n",
    "The number of hidden units a parameter of the network, often called a **hyperparameter** to differentiate it from the weights and biases parameters. As you'll see later when we discuss training a neural network, the more hidden units a network has, and the more layers, the better able it is to learn from data and make accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy to Torch and back\n",
    "\n",
    "Special bonus section! PyTorch has a great feature for converting between Numpy arrays and Torch tensors. To create a tensor from a Numpy array, use `torch.from_numpy()`. To convert a tensor to a Numpy array, use the `.numpy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.random.rand(4,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory is shared between the Numpy array and Torch tensor, so if you change the values in-place of one object, the other will change as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply PyTorch Tensor by 2, in place\n",
    "b.mul_(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy array matches new values from Tensor\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
